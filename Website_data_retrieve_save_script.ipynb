{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First - You must install all the required libraries mentioned in \"requirements.txt\" file - One time requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - To install libraries -> Run \"requirements.txt\" file\n",
    "##### - Command to run in Terminal -> pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - If not able to run in terminal, go to \"requirements.txt\" file in your directory and copy each library and run individually in terminal and install using command - pip install \"library\"\n",
    "- E.g. pip install selenium=4.15.2\n",
    "##### Mark Woodamsnee edit\n",
    "##### Or you can do it in this file only with command E.g. !pip install selenium=4.15.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import webdriver_manager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution - This file you should Run only once in a month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code to retrieve data from 8 websites into .csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To run the cell - select the particular cell and press Ctrl+Enter or you can choose from left arrow button on the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's month: September\n"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "#options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument('--disable-gpu')\n",
    "options.page_load_strategy = \"none\"\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "# Get the month name from today's date\n",
    "month_name = today.strftime('%B')\n",
    "\n",
    "print(\"Today's month:\", month_name)\n",
    "\n",
    "keywords_ignore=['Teaching', 'Instructor','Research Scientist','Lecturer','Research Assistant','Research Engineer','Teaching Professor',\n",
    "                 'Teaching Assistant Professor','Emeritus','Emerita','Professor Emeritus','Professor Emerita','Adjunct','Adjunct Professor',\n",
    "                 'Professor of the Practice','Coordinator','Instrument Maker','System Support Engineer','Academic Advisor',\n",
    "                 'Mechanical Engr','Communications Officer','Research Technician','Research Investigator','Visiting Assistant Professor',\n",
    "                 'Department Affiliate','Director of Human Resources','Manager','Courier','Helluva Engineer','HR Consultant','Academic Assistant',\n",
    "                 'Support Professional','Academic Professional Chair in Communication Skills','Financial Administrator','Senior Engineer',\n",
    "                 'Director IT','Associate Chair for Inclusive Excellence','Administrative Professional','Director of Student Engagement and Success',\n",
    "                 'Director of Business Operations','Mail Clerk','Director of Design & Innovation','Financial Admin','Administrator Lead',\n",
    "                 'Director of Development','Electrical Engineer','Administrative Supervisor','Support Prof','Director of Laboratory Development',\n",
    "                 'Machine Shop Supervisor','Senior Academic Professional','Academic Professional','Facilities Assistant','Admin Professional',\n",
    "                 'Mechanical Engineer','Communications Officer','Financial Mgr','Grants Administrator','Development Assistant',\n",
    "                 'Application Developer','Administrative Officer','Visiting Professor','Mechanical Technician','Visiting Scholar']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All functions for 8 websites\n",
    "\n",
    "def Caltech(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#    options = Options()\n",
    "#    options.add_experimental_option(\"detach\", True)\n",
    "#    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"https://mce.caltech.edu/people\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('div', class_='person-teaser__title')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('div', class_='person-teaser__job-title')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    os.makedirs(month_name)\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'caltechprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'caltechprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "    \n",
    "    df.to_csv('./'+month_name+'/'+'caltechprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "    \n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "    \n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "    os.mkdir('./'+month_name+'/'+'files')\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/caltechprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Caltech')\n",
    "\n",
    "\n",
    "def Georgia(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://www.me.gatech.edu/faculty\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    def data(driver):\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find all div elements with class \"name\" (containing professor names)\n",
    "        names = soup.find_all('div', class_='faculty-name')\n",
    "\n",
    "        # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "        titles = soup.find_all('div', class_='faculty-title')\n",
    "\n",
    "        # Iterate over the professors and print their names and profiles\n",
    "        for name, title in zip(names, titles):\n",
    "            professor_names.append(name.text.strip())\n",
    "            professor_profiles.append(title.text.strip())\n",
    "        \n",
    "        return professor_names,professor_profiles\n",
    "\n",
    "    count=1\n",
    "    for i in range(50):\n",
    "        professor_names,professor_profiles = data(driver)\n",
    "        print(len(professor_names))\n",
    "\n",
    "        try:\n",
    "            print(count)\n",
    "\n",
    "            next_button =driver.find_element(By.CSS_SELECTOR, \"a[title='Go to next page']\")\n",
    "            # Click on the \"Next\" button\n",
    "            ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "\n",
    "            # Wait for the page to load\n",
    "            time.sleep(10)\n",
    "\n",
    "            # Increment the count\n",
    "            count += 1\n",
    "        except:\n",
    "            print(\"Next button not found or reached end of pages\")\n",
    "            break\n",
    "\n",
    "\n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'georgiaprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'georgiaprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'georgiaprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "    \n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "    \n",
    "    df.to_csv('./'+month_name+'/'+'files/georgiaprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Georgia')\n",
    "\n",
    "\n",
    "def Michigan(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://me.engin.umich.edu/people/faculty/\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('span', class_='faculty-name')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('span', class_='faculty-titles')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'michiganprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'michiganprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'michiganprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "    \n",
    "    df.to_csv('./'+month_name+'/'+'files/michiganprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Michigan')\n",
    "\n",
    "def MIT(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://meche.mit.edu/people\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('span', class_='name')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('span', class_='title')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'mitprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'mitprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'mitprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/mitprofessors_prof_category.csv',index=False)\n",
    "    print('Done for MIT')\n",
    "\n",
    "def Purdue(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://engineering.purdue.edu/ME/People/faculty.html\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('div', class_='col col-xs-12 col-md-7 list-name')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('div', class_='people-list-title')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    a = lambda prof: prof.split('\\n')[0]\n",
    "    professors_df['Professor Name'] = professors_df['Professor Name'].apply(a)\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'purdueprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'purdueprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'purdueprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "    \n",
    "    df.to_csv('./'+month_name+'/'+'files/purdueprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Purdue')\n",
    "\n",
    "def Standford(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://me.stanford.edu/people/faculty\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('span', class_='field-content')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('div', class_='field-content')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'stanfordprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'stanfordprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "        \n",
    "    df.to_csv('./'+month_name+'/'+'stanfordprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/stanfordprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Stanford')\n",
    "\n",
    "def UC_Berkeley(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://me.berkeley.edu/faculty/\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    a_tags = soup.find_all('a', class_='_self cvplbd')\n",
    "    href_list = []\n",
    "\n",
    "    # Extract the href attribute from each <a> tag and store in the list\n",
    "    for a_tag in a_tags:\n",
    "        href_list.append(a_tag['href'])\n",
    "\n",
    "\n",
    "    professor_names=[]\n",
    "    professor_profiles=[]\n",
    "    print(len(href_list))\n",
    "\n",
    "    for i,link in enumerate(href_list):\n",
    "        print(i, href_list[i])\n",
    "        options = webdriver.ChromeOptions()\n",
    "        #options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument(\"--headless\")\n",
    "        #options.page_load_strategy = \"none\"\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "        driver.get(link)\n",
    "        \n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        names = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//h1[@class=\"entry-title\"]')))\n",
    "\n",
    "\n",
    "        titles = driver.find_element(\"tag name\", \"h3\")\n",
    "\n",
    "        # Find the p tag within the h3 tag\n",
    "        if i == 12:\n",
    "            print(\"skipped i = 12\")\n",
    "        else:\n",
    "            p_element = titles.find_element(\"css selector\", \"p\")\n",
    "\n",
    "            professor_names.append(names.text)\n",
    "            professor_profiles.append(p_element.text)\n",
    "        \n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'uc_berkeleyprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'uc_berkeleyprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "        \n",
    "    df.to_csv('./'+month_name+'/'+'uc_berkeleyprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/uc_berkeleyprofessors_prof_category.csv',index=False)\n",
    "    print('Done for UC_berkeley')\n",
    "\n",
    "def UIUC(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://mechse.illinois.edu/people/faculty/all-faculty\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('div', class_='name')\n",
    "    print(len(names))\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('div', class_='title')\n",
    "    print(len(titles))\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'UIUCprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'UIUCprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'UIUCprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "    \n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/UIUCprofessors_prof_category.csv',index=False)\n",
    "    print('Done for UIUC')\n",
    "\n",
    "    print('Done saving all files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Caltech(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Georgia(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Michigan(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIT(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Purdue(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Standford(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "0 https://me.berkeley.edu/people/m-reza-alam/\n",
      "1 https://me.berkeley.edu/people/francesco-borrelli/\n",
      "2 https://me.berkeley.edu/people/van-p-carey/\n",
      "3 https://me.berkeley.edu/people/james-casey/\n",
      "4 https://me.berkeley.edu/people/chris-dames/\n",
      "5 https://me.berkeley.edu/people/michael-frenklach/\n",
      "6 https://me.berkeley.edu/people/michael-gollner/\n",
      "7 https://me.berkeley.edu/people/kosa-goucher-lambert/\n",
      "8 https://me.berkeley.edu/people/costas-grigoropoulos/\n",
      "9 https://me.berkeley.edu/people/grace-x-gu/\n",
      "10 https://me.berkeley.edu/people/roberto-horowitz/\n",
      "11 https://me.berkeley.edu/people/alexis-kaminski/\n",
      "12 https://me.berkeley.edu/people/ken-kamrin/\n",
      "skipped i = 12\n",
      "13 https://me.berkeley.edu/people/homayoon-kazerooni/\n",
      "14 https://me.berkeley.edu/people/tony-m-keaveny/\n",
      "15 https://me.berkeley.edu/people/kyriakos-komvopoulos/\n",
      "16 https://me.berkeley.edu/people/liwei-lin/\n",
      "17 https://me.berkeley.edu/people/fai-ma/\n",
      "18 https://me.berkeley.edu/people/simo-makiharju/\n",
      "19 https://me.berkeley.edu/people/philip-s-marcus/\n",
      "20 https://me.berkeley.edu/people/sara-mcmains/\n",
      "21 https://me.berkeley.edu/people/negar-mehr/\n",
      "22 https://me.berkeley.edu/people/mohammad-r-k-mofrad/\n",
      "23 https://me.berkeley.edu/people/mark-w-mueller/\n",
      "24 https://me.berkeley.edu/people/grace-d-oconnell/\n",
      "25 https://me.berkeley.edu/people/oliver-m-oreilly/\n",
      "26 https://me.berkeley.edu/people/panayiotis-papadopoulos/\n",
      "27 https://me.berkeley.edu/people/kameshwar-poolla/\n",
      "28 https://me.berkeley.edu/people/lisa-pruitt/\n",
      "29 https://me.berkeley.edu/people/omer-savas/\n",
      "30 https://me.berkeley.edu/people/thomas-schutzius/\n",
      "31 https://me.berkeley.edu/people/shawn-shadden/\n",
      "32 https://me.berkeley.edu/people/lydia-sohn/\n",
      "33 https://me.berkeley.edu/people/koushil-sreenath/\n",
      "34 https://me.berkeley.edu/people/david-steigmann/\n",
      "35 https://me.berkeley.edu/people/hannah-stuart/\n",
      "36 https://me.berkeley.edu/people/hayden-taylor/\n",
      "37 https://me.berkeley.edu/people/masayoshi-tomizuka/\n",
      "38 https://me.berkeley.edu/people/lining-yao/\n",
      "39 https://me.berkeley.edu/people/tarek-i-zohdi/\n",
      "40 https://me.berkeley.edu/people/murat-arcak/\n",
      "41 https://me.berkeley.edu/people/saikat-chaudhuri/\n",
      "42 https://me.berkeley.edu/people/peter-hosemann/\n",
      "43 https://me.berkeley.edu/people/dorian-liepmann/\n",
      "44 https://me.berkeley.edu/people/robert-o-ritchie/\n",
      "45 https://me.berkeley.edu/people/s-shankar-sastry/\n",
      "46 https://me.berkeley.edu/people/somayeh-sojoudi/\n",
      "47 https://me.berkeley.edu/people/alice-m-agogino/\n",
      "48 https://me.berkeley.edu/people/david-b-bogy/\n",
      "49 https://me.berkeley.edu/people/carlos-fernandez-pello/\n",
      "50 https://me.berkeley.edu/people/boris-rubinsky/\n",
      "51 https://me.berkeley.edu/people/david-m-auslander/\n",
      "52 https://me.berkeley.edu/people/jyh-yuan-chen/\n",
      "53 https://me.berkeley.edu/people/robert-dibble/\n",
      "54 https://me.berkeley.edu/people/ralph-greif/\n",
      "55 https://me.berkeley.edu/people/george-johnson/\n",
      "56 https://me.berkeley.edu/people/george-leitmann/\n",
      "57 https://me.berkeley.edu/people/dennis-k-lieu/\n",
      "58 https://me.berkeley.edu/people/stephen-morris/\n",
      "59 https://me.berkeley.edu/people/patrick-j-pagni/\n",
      "60 https://me.berkeley.edu/people/benson-h-tongue/\n",
      "61 https://me.berkeley.edu/people/paul-k-wright/\n",
      "62 https://me.berkeley.edu/people/kazuo-yamazaki/\n",
      "63 https://me.berkeley.edu/people/ronald-w-yeung/\n",
      "64 https://me.berkeley.edu/people/xiang-zhang/\n",
      "65 https://me.berkeley.edu/people/george-anwar/\n",
      "66 https://me.berkeley.edu/people/gabriel-gomes/\n",
      "67 https://me.berkeley.edu/people/ala-moradian/\n",
      "68 https://me.berkeley.edu/people/k-youssefi/\n",
      "69 https://me.berkeley.edu/people/david-horsley/\n",
      "70 https://me.berkeley.edu/people/samuel-mao/\n",
      "71 https://me.berkeley.edu/people/ravi-prasher/\n",
      "72 https://me.berkeley.edu/people/vassilia-zorba/\n",
      "Done for UC_berkeley\n"
     ]
    }
   ],
   "source": [
    "UC_Berkeley(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "#options.add_argument(\"--start-maximized\")\n",
    "#options.add_argument(\"--headless\")\n",
    "options.add_argument('--disable-gpu')\n",
    "options.page_load_strategy = \"none\"\n",
    "\n",
    "UIUC(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Sometimes This gives error in fetching data, so rerun the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's Done!\n",
    "#### You can check in your directory - \"Month\" folder and inside it \"files\" folder as filtered files for each website which are further used for creating graphs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
