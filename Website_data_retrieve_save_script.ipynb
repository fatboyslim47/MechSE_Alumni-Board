{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First - You must install all the required libraries mentioned in \"requirements.txt\" file - One time requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - To install libraries -> Run \"requirements.txt\" file\n",
    "##### - Command to run in Terminal -> pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - If not able to run in terminal, go to \"requirements.txt\" file in your directory and copy each library and run individually in terminal and install using command - pip install \"library\"\n",
    "- E.g. pip install selenium=4.15.2\n",
    "##### Mark Woodamsnee edit\n",
    "##### Or you can do it in this file only with command E.g. !pip install selenium=4.15.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import webdriver_manager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution - This file you should Run only once in a month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code to retrieve data from 8 websites into .csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To run the cell - select the particular cell and press Ctrl+Enter or you can choose from left arrow button on the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's month: August\n"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "#options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument('--disable-gpu')\n",
    "options.page_load_strategy = \"none\"\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "# Get the month name from today's date\n",
    "month_name = today.strftime('%B')\n",
    "\n",
    "print(\"Today's month:\", month_name)\n",
    "\n",
    "keywords_ignore=['Teaching', 'Instructor','Research Scientist','Lecturer','Research Assistant','Research Engineer','Teaching Professor',\n",
    "                 'Teaching Assistant Professor','Emeritus','Emerita','Professor Emeritus','Professor Emerita','Adjunct','Adjunct Professor',\n",
    "                 'Professor of the Practice','Coordinator','Instrument Maker','System Support Engineer','Academic Advisor',\n",
    "                 'Mechanical Engr','Communications Officer','Research Technician','Research Investigator','Visiting Assistant Professor',\n",
    "                 'Department Affiliate','Director of Human Resources','Manager','Courier','Helluva Engineer','HR Consultant','Academic Assistant',\n",
    "                 'Support Professional','Academic Professional Chair in Communication Skills','Financial Administrator','Senior Engineer',\n",
    "                 'Director IT','Associate Chair for Inclusive Excellence','Administrative Professional','Director of Student Engagement and Success',\n",
    "                 'Director of Business Operations','Mail Clerk','Director of Design & Innovation','Financial Admin','Administrator Lead',\n",
    "                 'Director of Development','Electrical Engineer','Administrative Supervisor','Support Prof','Director of Laboratory Development',\n",
    "                 'Machine Shop Supervisor','Senior Academic Professional','Academic Professional','Facilities Assistant','Admin Professional',\n",
    "                 'Mechanical Engineer','Communications Officer','Financial Mgr','Grants Administrator','Development Assistant',\n",
    "                 'Application Developer','Administrative Officer','Visiting Professor','Mechanical Technician','Visiting Scholar']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All functions for 8 websites\n",
    "\n",
    "def Caltech(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://mce.caltech.edu/people\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('div', class_='person-teaser__title')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('div', class_='person-teaser__job-title')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    os.makedirs(month_name)\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'caltechprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'caltechprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "    \n",
    "    df.to_csv('./'+month_name+'/'+'caltechprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "    \n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "    \n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "    os.mkdir('./'+month_name+'/'+'files')\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/caltechprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Caltech')\n",
    "\n",
    "\n",
    "def Georgia(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://www.me.gatech.edu/faculty\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    def data(driver):\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find all div elements with class \"name\" (containing professor names)\n",
    "        names = soup.find_all('div', class_='faculty-name')\n",
    "\n",
    "        # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "        titles = soup.find_all('div', class_='faculty-title')\n",
    "\n",
    "        # Iterate over the professors and print their names and profiles\n",
    "        for name, title in zip(names, titles):\n",
    "            professor_names.append(name.text.strip())\n",
    "            professor_profiles.append(title.text.strip())\n",
    "        \n",
    "        return professor_names,professor_profiles\n",
    "\n",
    "    count=1\n",
    "    for i in range(50):\n",
    "        professor_names,professor_profiles = data(driver)\n",
    "        print(len(professor_names))\n",
    "\n",
    "        try:\n",
    "            print(count)\n",
    "\n",
    "            next_button =driver.find_element(By.CSS_SELECTOR, \"a[title='Go to next page']\")\n",
    "            # Click on the \"Next\" button\n",
    "            ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "\n",
    "            # Wait for the page to load\n",
    "            time.sleep(10)\n",
    "\n",
    "            # Increment the count\n",
    "            count += 1\n",
    "        except:\n",
    "            print(\"Next button not found or reached end of pages\")\n",
    "            break\n",
    "\n",
    "\n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'georgiaprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'georgiaprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'georgiaprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "    \n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "    \n",
    "    df.to_csv('./'+month_name+'/'+'files/georgiaprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Georgia')\n",
    "\n",
    "\n",
    "def Michigan(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://me.engin.umich.edu/people/faculty/\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('span', class_='faculty-name')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('span', class_='faculty-titles')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'michiganprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'michiganprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'michiganprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "    \n",
    "    df.to_csv('./'+month_name+'/'+'files/michiganprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Michigan')\n",
    "\n",
    "def MIT(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://meche.mit.edu/people\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('span', class_='name')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('span', class_='title')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'mitprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'mitprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'mitprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/mitprofessors_prof_category.csv',index=False)\n",
    "    print('Done for MIT')\n",
    "\n",
    "def Purdue(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://engineering.purdue.edu/ME/People/faculty.html\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('div', class_='col col-xs-12 col-md-7 list-name')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('div', class_='people-list-title')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    a = lambda prof: prof.split('\\n')[0]\n",
    "    professors_df['Professor Name'] = professors_df['Professor Name'].apply(a)\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'purdueprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'purdueprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'purdueprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "    \n",
    "    df.to_csv('./'+month_name+'/'+'files/purdueprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Purdue')\n",
    "\n",
    "def Standford(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://me.stanford.edu/people/faculty\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('span', class_='field-content')\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('div', class_='field-content')\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'stanfordprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'stanfordprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "        \n",
    "    df.to_csv('./'+month_name+'/'+'stanfordprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/stanfordprofessors_prof_category.csv',index=False)\n",
    "    print('Done for Stanford')\n",
    "\n",
    "def UC_Berkeley(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://me.berkeley.edu/faculty/\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    a_tags = soup.find_all('a', class_='_self cvplbd')\n",
    "    href_list = []\n",
    "\n",
    "    # Extract the href attribute from each <a> tag and store in the list\n",
    "    for a_tag in a_tags:\n",
    "        href_list.append(a_tag['href'])\n",
    "\n",
    "\n",
    "    professor_names=[]\n",
    "    professor_profiles=[]\n",
    "    print(len(href_list))\n",
    "\n",
    "    for i,link in enumerate(href_list):\n",
    "        print(i)\n",
    "        options = webdriver.ChromeOptions()\n",
    "        #options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument(\"--headless\")\n",
    "        #options.page_load_strategy = \"none\"\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "        driver.get(link)\n",
    "        \n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        names = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//h1[@class=\"entry-title\"]')))\n",
    "\n",
    "\n",
    "        titles = driver.find_element(\"tag name\", \"h3\")\n",
    "\n",
    "        # Find the p tag within the h3 tag\n",
    "        p_element = titles.find_element(\"css selector\", \"p\")\n",
    "\n",
    "        professor_names.append(names.text)\n",
    "        professor_profiles.append(p_element.text)\n",
    "        \n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'uc_berkeleyprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'uc_berkeleyprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "        \n",
    "    df.to_csv('./'+month_name+'/'+'uc_berkeleyprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/uc_berkeleyprofessors_prof_category.csv',index=False)\n",
    "    print('Done for UC_berkeley')\n",
    "\n",
    "def UIUC(options,month_name,keywords_ignore):\n",
    "\n",
    "    # Initialize the WebDriver using ChromeDriverManager\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(\"https://mechse.illinois.edu/people/faculty/all-faculty\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"name\" (containing professor names)\n",
    "    names = soup.find_all('div', class_='name')\n",
    "    print(len(names))\n",
    "\n",
    "    # Find all div elements with class \"title\" (containing professor titles/profiles)\n",
    "    titles = soup.find_all('div', class_='title')\n",
    "    print(len(titles))\n",
    "\n",
    "    professor_names = []\n",
    "    professor_profiles = []\n",
    "\n",
    "    # Iterate over the professors and print their names and profiles\n",
    "    for name, title in zip(names, titles):\n",
    "        professor_names.append(name.text.strip())\n",
    "        professor_profiles.append(title.text.strip())\n",
    "        \n",
    "    # Create a dataframe to store the data\n",
    "    professors_df = pd.DataFrame({'Professor Name': professor_names, 'Professor Profile': professor_profiles})\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    professors_df.to_csv('./'+month_name+'/'+'UIUCprofessors.csv',index=False)\n",
    "\n",
    "    df=pd.read_csv('./'+month_name+'/'+'UIUCprofessors.csv')\n",
    "\n",
    "    for keyword in keywords_ignore:\n",
    "        mask = df['Professor Profile'].str.contains(rf'\\b{keyword}\\b', regex=True,na=False)\n",
    "        df=df[~mask]\n",
    "\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'UIUCprofessors_filter.csv',index=False)\n",
    "\n",
    "    df['Professor Profile'] = df['Professor Profile'].fillna('')\n",
    "    keys = ['Assistant Professor','ASST PROF','Asst Professor']\n",
    "    keys1 = ['Associate Professor','ASSOC PROF','Assoc Professor']\n",
    "    df['Assistant_Professor']=df['Professor Profile'].apply(lambda x: any(key in x for key in keys))\n",
    "    df['Associate_Professor']=df['Professor Profile'].apply(lambda x: any(keyw in x for keyw in keys1))\n",
    "    \n",
    "\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'Assistant_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Assistant Professor'\n",
    "        elif df.loc[i,'Associate_Professor']==1:\n",
    "            df.loc[i,'Faculty']='Associate Professor'\n",
    "        else:\n",
    "            df.loc[i,'Faculty']='Professor'\n",
    "\n",
    "    df=df[['Professor Name','Professor Profile','Faculty']]\n",
    "\n",
    "    df.to_csv('./'+month_name+'/'+'files/UIUCprofessors_prof_category.csv',index=False)\n",
    "    print('Done for UIUC')\n",
    "\n",
    "    print('Done saving all files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for Caltech\n"
     ]
    }
   ],
   "source": [
    "Caltech(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "1\n",
      "48\n",
      "2\n",
      "72\n",
      "3\n",
      "96\n",
      "4\n",
      "120\n",
      "5\n",
      "144\n",
      "6\n",
      "168\n",
      "7\n",
      "192\n",
      "8\n",
      "216\n",
      "9\n",
      "240\n",
      "10\n",
      "264\n",
      "11\n",
      "288\n",
      "12\n",
      "293\n",
      "13\n",
      "Next button not found or reached end of pages\n",
      "Done for Georgia\n"
     ]
    }
   ],
   "source": [
    "Georgia(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for Michigan\n"
     ]
    }
   ],
   "source": [
    "Michigan(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for MIT\n"
     ]
    }
   ],
   "source": [
    "MIT(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for Purdue\n"
     ]
    }
   ],
   "source": [
    "Purdue(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for Stanford\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Standford(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "Done for UC_berkeley\n"
     ]
    }
   ],
   "source": [
    "UC_Berkeley(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "154\n",
      "Done for UIUC\n",
      "Done saving all files\n"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "#options.add_argument(\"--start-maximized\")\n",
    "#options.add_argument(\"--headless\")\n",
    "options.add_argument('--disable-gpu')\n",
    "options.page_load_strategy = \"none\"\n",
    "\n",
    "UIUC(options,month_name,keywords_ignore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Sometimes This gives error in fetching data, so rerun the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's Done!\n",
    "#### You can check in your directory - \"Month\" folder and inside it \"files\" folder as filtered files for each website which are further used for creating graphs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
